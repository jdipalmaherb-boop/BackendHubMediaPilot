import { execFile } from 'child_process';
import { promisify } from 'util';
import { promises as fs } from 'fs';
import path from 'path';
import { env } from '../env';

const execFileAsync = promisify(execFile);

export interface CaptionSegment {
  start: number; // seconds
  end: number;   // seconds
  text: string;
}

export interface CaptionResult {
  segments: CaptionSegment[];
  srtContent: string;
  language?: string;
  confidence?: number;
}

export interface CaptionOptions {
  language?: string;
  model?: 'whisper' | 'gpt-5';
  burnIntoVideo?: boolean;
  generateSrt?: boolean;
}

// Step 1: Extract audio to WAV using FFmpeg
export async function extractAudioToWav(videoPath: string, outputPath: string): Promise<void> {
  const args = [
    '-y',
    '-i', videoPath,
    '-vn', // no video
    '-acodec', 'pcm_s16le', // 16-bit PCM
    '-ar', '16000', // 16kHz sample rate (optimal for speech recognition)
    '-ac', '1', // mono
    outputPath
  ];

  await execFileAsync('ffmpeg', args, {
    windowsHide: true,
    maxBuffer: 10 * 1024 * 1024,
  });
}

// Step 2: Speech-to-text using Whisper (if available) or GPT-5 stub
export async function speechToText(audioPath: string, options: CaptionOptions = {}): Promise<CaptionResult> {
  const { language = 'en', model = 'whisper' } = options;

  if (model === 'whisper') {
    return await whisperSpeechToText(audioPath, language);
  } else {
    return await gpt5SpeechToText(audioPath, language);
  }
}

// Whisper speech-to-text implementation
async function whisperSpeechToText(audioPath: string, language: string): Promise<CaptionResult> {
  try {
    // Try to use whisper command line tool
    const args = [
      audioPath,
      '--language', language,
      '--output_format', 'srt',
      '--output_dir', path.dirname(audioPath)
    ];

    await execFileAsync('whisper', args, {
      windowsHide: true,
      maxBuffer: 10 * 1024 * 1024,
    });

    // Read the generated SRT file
    const srtPath = audioPath.replace(/\.[^/.]+$/, '.srt');
    const srtContent = await fs.readFile(srtPath, 'utf-8');
    
    // Parse SRT content to extract segments
    const segments = parseSrtContent(srtContent);

    return {
      segments,
      srtContent,
      language,
      confidence: 0.9 // Whisper typically has high confidence
    };
  } catch (error) {
    console.warn('Whisper not available, falling back to GPT-5 stub:', error);
    return await gpt5SpeechToText(audioPath, language);
  }
}

// GPT-5 stub for speech-to-text (placeholder implementation)
async function gpt5SpeechToText(audioPath: string, language: string): Promise<CaptionResult> {
  // This is a stub implementation that would call GPT-5 for speech-to-text
  // In a real implementation, you would:
  // 1. Convert audio to base64 or upload to a temporary service
  // 2. Call GPT-5 API with audio data
  // 3. Parse the response to extract timestamps and text
  
  console.log(`[STUB] GPT-5 speech-to-text for ${audioPath} in ${language}`);
  
  // For now, return a mock result
  const mockSegments: CaptionSegment[] = [
    { start: 0, end: 3, text: 'Hello, this is a test caption.' },
    { start: 3, end: 6, text: 'Generated by GPT-5 speech-to-text.' },
    { start: 6, end: 9, text: 'This is a placeholder implementation.' }
  ];

  const srtContent = generateSrtContent(mockSegments);

  return {
    segments: mockSegments,
    srtContent,
    language,
    confidence: 0.7 // Lower confidence for stub
  };
}

// Step 3: Generate SRT file content
export function generateSrtContent(segments: CaptionSegment[]): string {
  return segments.map((segment, index) => {
    const startTime = formatSrtTime(segment.start);
    const endTime = formatSrtTime(segment.end);
    return `${index + 1}\n${startTime} --> ${endTime}\n${segment.text}\n`;
  }).join('\n');
}

// Helper function to format time for SRT format
function formatSrtTime(seconds: number): string {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const milliseconds = Math.floor((seconds % 1) * 1000);

  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${milliseconds.toString().padStart(3, '0')}`;
}

// Helper function to parse SRT content
function parseSrtContent(srtContent: string): CaptionSegment[] {
  const segments: CaptionSegment[] = [];
  const blocks = srtContent.trim().split(/\n\s*\n/);

  for (const block of blocks) {
    const lines = block.trim().split('\n');
    if (lines.length < 3) continue;

    const timeLine = lines[1];
    const text = lines.slice(2).join('\n');

    const timeMatch = timeLine.match(/(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})/);
    if (!timeMatch) continue;

    const startTime = parseSrtTime(timeMatch[1]);
    const endTime = parseSrtTime(timeMatch[2]);

    segments.push({
      start: startTime,
      end: endTime,
      text: text.trim()
    });
  }

  return segments;
}

// Helper function to parse SRT time format
function parseSrtTime(timeStr: string): number {
  const [time, milliseconds] = timeStr.split(',');
  const [hours, minutes, seconds] = time.split(':').map(Number);
  
  return hours * 3600 + minutes * 60 + seconds + Number(milliseconds) / 1000;
}

// Complete caption pipeline
export async function processCaptions(
  videoPath: string,
  outputDir: string,
  options: CaptionOptions = {}
): Promise<{
  srtPath?: string;
  result: CaptionResult;
}> {
  const { generateSrt = true, burnIntoVideo = false } = options;
  
  // Step 1: Extract audio
  const audioPath = path.join(outputDir, 'audio.wav');
  await extractAudioToWav(videoPath, audioPath);

  // Step 2: Speech-to-text
  const result = await speechToText(audioPath, options);

  // Step 3: Generate SRT file if requested
  let srtPath: string | undefined;
  if (generateSrt) {
    srtPath = path.join(outputDir, 'captions.srt');
    await fs.writeFile(srtPath, result.srtContent, 'utf-8');
  }

  // Clean up audio file
  try {
    await fs.unlink(audioPath);
  } catch (error) {
    console.warn('Failed to clean up audio file:', error);
  }

  return {
    srtPath,
    result
  };
}

// Helper function to validate caption segments
export function validateCaptionSegments(segments: CaptionSegment[]): boolean {
  if (!Array.isArray(segments) || segments.length === 0) {
    return false;
  }

  for (const segment of segments) {
    if (typeof segment.start !== 'number' || 
        typeof segment.end !== 'number' || 
        typeof segment.text !== 'string' ||
        segment.start < 0 || 
        segment.end <= segment.start ||
        segment.text.trim().length === 0) {
      return false;
    }
  }

  return true;
}

// Helper function to merge overlapping segments
export function mergeOverlappingSegments(segments: CaptionSegment[]): CaptionSegment[] {
  if (segments.length <= 1) return segments;

  const sorted = [...segments].sort((a, b) => a.start - b.start);
  const merged: CaptionSegment[] = [];

  for (const segment of sorted) {
    const last = merged[merged.length - 1];
    
    if (!last || segment.start > last.end) {
      merged.push(segment);
    } else {
      // Merge overlapping segments
      last.end = Math.max(last.end, segment.end);
      last.text = `${last.text} ${segment.text}`.trim();
    }
  }

  return merged;
}
